{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINAL PROJECT : CIS9650 \n",
    "#### (FP-GROUP-06)\n",
    "#### PARTICIPANTS: JAYANT BISHNOI , STANLEY CELESTIN, CHUNMIAO CHEN.\n",
    "\n",
    "#### AIM: TO BE ABLE TO SCRAPE A WEBPAGE AND PARSE IT TO FIND MEANINGFUL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = \"http://books.toscrape.com/index.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "result = requests.get(main_url)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(result.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!-->\n",
      "<html class=\"no-js\" lang=\"en-us\">\n",
      " <!--<![endif]-->\n",
      " <head>\n",
      "  <title>\n",
      "   All products | Books to Scrape - Sandbox\n",
      "  </title>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\n",
      "  <meta content=\"24th Jun 2016 09:29\" name=\"created\"/>\n",
      "  <meta content=\"\" name=\"description\"/>\n",
      "  <meta content=\"width=device-width\" name=\"viewport\"/>\n",
      "  <meta content=\"NOARCHIVE,NOCACHE\" name=\"robots\"/>\n",
      "  <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->\n",
      "  <!--[if lt IE 9]>\n",
      "        <script src=\"//html5shim.googlecode.com/svn/trunk/html5.js\"></script>\n",
      "        <![endif]-->\n",
      "  <link href=\"static/oscar/favicon.ico\" rel=\"shortcut icon\"/>\n",
      "  <link href=\"static/oscar/css/styles.css\" rel=\"stylesheet\" type=\"tex\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    All products | Books to Scrape - Sandbox\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To find the title from html page\n",
    "soup.html.title.get_text()\n",
    "#soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag: <h1>All products</h1>\n",
      "link:  None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Find All 'h1' tag/link from the website\n",
    "for tag in soup.findAll('h1'):\n",
    "    print('tag:', tag)\n",
    "    print('link: ', tag.get('class'),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siblings:  <small> We love being scraped!</small>\n",
      "siblings:  '\\n'\n"
     ]
    }
   ],
   "source": [
    "## Find all siblings\n",
    "for sibling in soup.a.next_siblings:\n",
    "    print('siblings: ', repr(sibling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price: Â£51.77\n",
      "Price: Â£53.74\n",
      "Price: Â£50.10\n",
      "Price: Â£47.82\n",
      "Price: Â£54.23\n",
      "Price: Â£22.65\n",
      "Price: Â£33.34\n",
      "Price: Â£17.93\n",
      "Price: Â£22.60\n",
      "Price: Â£52.15\n",
      "Price: Â£13.99\n",
      "Price: Â£20.66\n",
      "Price: Â£17.46\n",
      "Price: Â£52.29\n",
      "Price: Â£35.02\n",
      "Price: Â£57.25\n",
      "Price: Â£23.88\n",
      "Price: Â£37.59\n",
      "Price: Â£51.33\n",
      "Price: Â£45.17\n"
     ]
    }
   ],
   "source": [
    "## Find the book price from the website, 20 books on page 1\n",
    "x=soup.find_all(\"p\", class_ = \"price_color\")\n",
    "for tag in x: \n",
    "    print(\"Price\", tag.get_text(), sep=': ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "A Light in the Attic\n",
      "None\n",
      "Tipping the Velvet\n",
      "None\n",
      "Soumission\n",
      "None\n",
      "Sharp Objects\n",
      "None\n",
      "Sapiens: A Brief History of Humankind\n",
      "None\n",
      "The Requiem Red\n",
      "None\n",
      "The Dirty Little Secrets of Getting Your Dream Job\n",
      "None\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "None\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "None\n",
      "The Black Maria\n",
      "None\n",
      "Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "None\n",
      "Shakespeare's Sonnets\n",
      "None\n",
      "Set Me Free\n",
      "None\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "None\n",
      "Rip it Up and Start Again\n",
      "None\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "None\n",
      "Olio\n",
      "None\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "None\n",
      "Libertarianism for Beginners\n",
      "None\n",
      "It's Only the Himalayas\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## Needs to remove None to get clear book titles\n",
    "x=soup.findAll(\"a\")\n",
    "for anchor in x:\n",
    "    print(anchor.get('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag: <h3><a href=\"catalogue/a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/tipping-the-velvet_999/index.html\" title=\"Tipping the Velvet\">Tipping the Velvet</a></h3>\n",
      "tag: <h3><a href=\"catalogue/soumission_998/index.html\" title=\"Soumission\">Soumission</a></h3>\n",
      "tag: <h3><a href=\"catalogue/sharp-objects_997/index.html\" title=\"Sharp Objects\">Sharp Objects</a></h3>\n",
      "tag: <h3><a href=\"catalogue/sapiens-a-brief-history-of-humankind_996/index.html\" title=\"Sapiens: A Brief History of Humankind\">Sapiens: A Brief History ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/the-requiem-red_995/index.html\" title=\"The Requiem Red\">The Requiem Red</a></h3>\n",
      "tag: <h3><a href=\"catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html\" title=\"The Dirty Little Secrets of Getting Your Dream Job\">The Dirty Little Secrets ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html\" title=\"The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\">The Coming Woman: A ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html\" title=\"The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\">The Boys in the ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/the-black-maria_991/index.html\" title=\"The Black Maria\">The Black Maria</a></h3>\n",
      "tag: <h3><a href=\"catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html\" title=\"Starving Hearts (Triangular Trade Trilogy, #1)\">Starving Hearts (Triangular Trade ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/shakespeares-sonnets_989/index.html\" title=\"Shakespeare's Sonnets\">Shakespeare's Sonnets</a></h3>\n",
      "tag: <h3><a href=\"catalogue/set-me-free_988/index.html\" title=\"Set Me Free\">Set Me Free</a></h3>\n",
      "tag: <h3><a href=\"catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html\" title=\"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\">Scott Pilgrim's Precious Little ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/rip-it-up-and-start-again_986/index.html\" title=\"Rip it Up and Start Again\">Rip it Up and ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html\" title=\"Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\">Our Band Could Be ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/olio_984/index.html\" title=\"Olio\">Olio</a></h3>\n",
      "tag: <h3><a href=\"catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html\" title=\"Mesaerion: The Best Science Fiction Stories 1800-1849\">Mesaerion: The Best Science ...</a></h3>\n",
      "tag: <h3><a href=\"catalogue/libertarianism-for-beginners_982/index.html\" title=\"Libertarianism for Beginners\">Libertarianism for Beginners</a></h3>\n",
      "tag: <h3><a href=\"catalogue/its-only-the-himalayas_981/index.html\" title=\"It's Only the Himalayas\">It's Only the Himalayas</a></h3>\n"
     ]
    }
   ],
   "source": [
    "## h3 tag contains all the book titles\n",
    "for h2 in soup.findAll('h3'):\n",
    "    print('tag:', h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "## Find the how many books rated as five star from the website, limited to page 1\n",
    "## star-rating Five;  star-rating Four;  star-rating Three;  star-rating Two;  star-rating One; \n",
    "x=soup.find_all(\"p\", class_ = \"star-rating Five\")\n",
    "\n",
    "# count starting 0; string variable equal to the book rate as how many stars in html class\n",
    "count=0\n",
    "st='star-rating Five'\n",
    "for st in x:\n",
    "    count+=1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to request and parse a HTML web page\n",
    "def getAndParseURL(url):\n",
    "    result = requests.get(url)\n",
    "    soup = BeautifulSoup(result.text, 'html.parser')\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'catalogue/a-light-in-the-attic_1000/index.html'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since the actual link of the book is under the anchor tag under <div> tag we will need to specify other child tags\n",
    "soup.find(\"article\", class_ = \"product_pod\").div.a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 fetched products URLs\n",
      "One example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['catalogue/a-light-in-the-attic_1000/index.html',\n",
       " 'catalogue/tipping-the-velvet_999/index.html',\n",
       " 'catalogue/soumission_998/index.html',\n",
       " 'catalogue/sharp-objects_997/index.html',\n",
       " 'catalogue/sapiens-a-brief-history-of-humankind_996/index.html']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we need to find the product links of all the books on the page\n",
    "main_page_products_urls = [x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")]\n",
    "\n",
    "print(str(len(main_page_products_urls)) + \" fetched products URLs\")\n",
    "print(\"One example:\")\n",
    "main_page_products_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous URLs correspond to their relative path from the main page,add before them the URL of the main page to make it complete\n",
    "def getBooksURLs(url):\n",
    "    soup = getAndParseURL(url)\n",
    "    # remove the index.html part of the base url before returning the results\n",
    "    return([\"/\".join(url.split(\"/\")[:-1]) + \"/\" + x.div.a.get('href') for x in soup.findAll(\"article\", class_ = \"product_pod\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 fetched categories URLs\n",
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://books.toscrape.com/index.htmlcatalogue/category/books/travel_2/index.html',\n",
       " 'http://books.toscrape.com/index.htmlcatalogue/category/books/mystery_3/index.html',\n",
       " 'http://books.toscrape.com/index.htmlcatalogue/category/books/historical-fiction_4/index.html',\n",
       " 'http://books.toscrape.com/index.htmlcatalogue/category/books/sequential-art_5/index.html',\n",
       " 'http://books.toscrape.com/index.htmlcatalogue/category/books/classics_6/index.html']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we use regular expresion tocombine a regular expression pattern into pattern objects, which can be used for pattern matching. \n",
    "#It also helps to search a pattern again without rewriting it.\n",
    "import re\n",
    "\n",
    "categories_urls = [main_url + x.get('href') for x in soup.find_all(\"a\", href=re.compile(\"catalogue/category/books\"))]\n",
    "categories_urls = categories_urls[1:] # we remove the first one because it corresponds to all the books\n",
    "\n",
    "print(str(len(categories_urls)) + \" fetched categories URLs\")\n",
    "print(\"Some examples:\")\n",
    "categories_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the results into a list\n",
    "pages_urls = [main_url]\n",
    "\n",
    "soup = getAndParseURL(pages_urls[0])\n",
    "\n",
    "# while we get two matches, this means that the web page contains a 'previous' and a 'next' button\n",
    "# if there is only one button, this means that we are either on the first page or on the last page\n",
    "# we stop when we get to the last page\n",
    "\n",
    "while len(soup.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(pages_urls) == 1:\n",
    "    \n",
    "    # get the new complete url by adding the fetched URL to the base URL (and removing the .html part of the base URL)\n",
    "    new_url = \"/\".join(pages_urls[-1].split(\"/\")[:-1]) + \"/\" + soup.findAll(\"a\", href=re.compile(\"page\"))[-1].get(\"href\")\n",
    "    \n",
    "    # add the URL to the list\n",
    "    pages_urls.append(new_url)\n",
    "    \n",
    "    # now,we parse the next page\n",
    "    soup = getAndParseURL(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 fetched URLs\n",
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://books.toscrape.com/index.html',\n",
       " 'http://books.toscrape.com/catalogue/page-2.html',\n",
       " 'http://books.toscrape.com/catalogue/page-3.html',\n",
       " 'http://books.toscrape.com/catalogue/page-4.html',\n",
       " 'http://books.toscrape.com/catalogue/page-5.html',\n",
       " 'http://books.toscrape.com/catalogue/page-6.html',\n",
       " 'http://books.toscrape.com/catalogue/page-7.html',\n",
       " 'http://books.toscrape.com/catalogue/page-8.html',\n",
       " 'http://books.toscrape.com/catalogue/page-9.html',\n",
       " 'http://books.toscrape.com/catalogue/page-10.html']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(len(pages_urls)) + \" fetched URLs\")\n",
    "print(\"Some examples:\")\n",
    "pages_urls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://books.toscrape.com/index.html\n",
      "http://books.toscrape.com/catalogue/page-2.html\n",
      "http://books.toscrape.com/catalogue/page-3.html\n",
      "http://books.toscrape.com/catalogue/page-4.html\n",
      "http://books.toscrape.com/catalogue/page-5.html\n",
      "http://books.toscrape.com/catalogue/page-6.html\n",
      "http://books.toscrape.com/catalogue/page-7.html\n",
      "http://books.toscrape.com/catalogue/page-8.html\n",
      "http://books.toscrape.com/catalogue/page-9.html\n",
      "http://books.toscrape.com/catalogue/page-10.html\n"
     ]
    }
   ],
   "source": [
    "## alternate way to get urls\n",
    "for i in pages_urls[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code for page 50: 200\n"
     ]
    }
   ],
   "source": [
    "#The 200 code indicates that there is no error.\n",
    "result = requests.get(\"http://books.toscrape.com/catalogue/page-50.html\")\n",
    "\n",
    "print(\"status code for page 50: \" + str(result.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use this information to get all our pages URLs: we should iterate until we get a 404 code(page not found)\n",
    "pages_urls = []\n",
    "\n",
    "new_page = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "while requests.get(new_page).status_code == 200:\n",
    "    pages_urls.append(new_page)\n",
    "    new_page = pages_urls[-1].split(\"-\")[0] + \"-\" + str(int(pages_urls[-1].split(\"-\")[1].split(\".\")[0]) + 1) + \".html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 fetched URLs\n",
      "First 5 Examples as below: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://books.toscrape.com/catalogue/page-1.html',\n",
       " 'http://books.toscrape.com/catalogue/page-2.html',\n",
       " 'http://books.toscrape.com/catalogue/page-3.html',\n",
       " 'http://books.toscrape.com/catalogue/page-4.html',\n",
       " 'http://books.toscrape.com/catalogue/page-5.html']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Modified\n",
    "print(str(len(pages_urls)) + \" fetched URLs\")\n",
    "print(\"First 5 Examples as below: \")\n",
    "pages_urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's get all our product URL'S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksURLs = []\n",
    "for page in pages_urls:\n",
    "    booksURLs.extend(getBooksURLs(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 fetched URLs\n",
      "Some examples:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',\n",
       " 'http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',\n",
       " 'http://books.toscrape.com/catalogue/soumission_998/index.html',\n",
       " 'http://books.toscrape.com/catalogue/sharp-objects_997/index.html',\n",
       " 'http://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(str(len(booksURLs)) + \" fetched URLs\")\n",
    "print(\"Some examples:\")\n",
    "booksURLs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOW WE CAN EXTRACT PRODUCT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#magic function to get wall time and CPU time\n",
    "\n",
    "names = []\n",
    "prices = []\n",
    "nb_in_stock = []\n",
    "img_urls = []\n",
    "categories = []\n",
    "ratings = []\n",
    "\n",
    "# scrape data for every book URL: this may take some time\n",
    "for url in booksURLs:\n",
    "    soup = getAndParseURL(url)\n",
    "    # product name\n",
    "    names.append(soup.find(\"div\", class_ = re.compile(\"product_main\")).h1.text)\n",
    "    # product price\n",
    "    prices.append(soup.find(\"p\", class_ = \"price_color\").text[2:]) # get rid of the pound sign\n",
    "    # number of available products\n",
    "    nb_in_stock.append(re.sub(\"[^0-9]\", \"\", soup.find(\"p\", class_ = \"instock availability\").text)) # get rid of non numerical characters\n",
    "    # image url\n",
    "    img_urls.append(url.replace(\"index.html\", \"\") + soup.find(\"img\").get(\"src\"))\n",
    "    # product category\n",
    "    categories.append(soup.find(\"a\", href = re.compile(\"../category/books/\")).get(\"href\").split(\"/\")[3])\n",
    "    # ratings\n",
    "    ratings.append(soup.find(\"p\", class_ = re.compile(\"star-rating\")).get(\"class\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>nb_in_stock</th>\n",
       "      <th>url_img</th>\n",
       "      <th>product_category</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>51.77</td>\n",
       "      <td>22</td>\n",
       "      <td>http://books.toscrape.com/catalogue/a-light-in...</td>\n",
       "      <td>poetry_23</td>\n",
       "      <td>Three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>20</td>\n",
       "      <td>http://books.toscrape.com/catalogue/tipping-th...</td>\n",
       "      <td>historical-fiction_4</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>20</td>\n",
       "      <td>http://books.toscrape.com/catalogue/soumission...</td>\n",
       "      <td>fiction_10</td>\n",
       "      <td>One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>20</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sharp-obje...</td>\n",
       "      <td>mystery_3</td>\n",
       "      <td>Four</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>54.23</td>\n",
       "      <td>20</td>\n",
       "      <td>http://books.toscrape.com/catalogue/sapiens-a-...</td>\n",
       "      <td>history_32</td>\n",
       "      <td>Five</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  price nb_in_stock  \\\n",
       "0                   A Light in the Attic  51.77          22   \n",
       "1                     Tipping the Velvet  53.74          20   \n",
       "2                             Soumission  50.10          20   \n",
       "3                          Sharp Objects  47.82          20   \n",
       "4  Sapiens: A Brief History of Humankind  54.23          20   \n",
       "\n",
       "                                             url_img      product_category  \\\n",
       "0  http://books.toscrape.com/catalogue/a-light-in...             poetry_23   \n",
       "1  http://books.toscrape.com/catalogue/tipping-th...  historical-fiction_4   \n",
       "2  http://books.toscrape.com/catalogue/soumission...            fiction_10   \n",
       "3  http://books.toscrape.com/catalogue/sharp-obje...             mystery_3   \n",
       "4  http://books.toscrape.com/catalogue/sapiens-a-...            history_32   \n",
       "\n",
       "  rating  \n",
       "0  Three  \n",
       "1    One  \n",
       "2    One  \n",
       "3   Four  \n",
       "4   Five  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add data into pandas df\n",
    "import pandas as pd\n",
    "\n",
    "scraped_data = pd.DataFrame({'name': names, 'price': prices, 'nb_in_stock': nb_in_stock, \"url_img\": img_urls, \"product_category\": categories, \"rating\": ratings})\n",
    "scraped_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
